{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_V2_lectura_de_un_libro(RNN)_libro(la_biblioteca_de_babel).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k6zNNLvrPR1f",
        "FshwyfFykB_4",
        "uIkOBKGAP0y-",
        "hFDzT0NtUpua",
        "SmC4IO8rg5TV",
        "SKuJocQwCLg1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8jPcr-tb-a5"
      },
      "source": [
        "#P0. Introducción -PLN\n",
        "\n",
        "---\n",
        "\n",
        "En la creación de redes neuronales necesitamos dos tipos de IA, para reconocer patrones o generar nuevos:\n",
        "\n",
        "*   Las que no tienen memoria, identifica el patrón y ya!...ejemplo las de visión artificial\n",
        "*   Las de memoria corta (Long Short Term Memory)...PLN\n",
        "*   Las que requieren mucha memoria (aprenden casi todo...BERT)...PLN y visión artificial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyasC25K4L-Y"
      },
      "source": [
        "**Caso de estudio: generación de texto**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Cualquier dato que se necesite procesar (sonido, imágenes, texto) primero debe ser convertido en un tensor numérico, un paso llamado “vectorización” (One-hot Encoding y WordEmbedding) de datos (y en nuestro ejemplo previamente las letras deben ser pasadas a valores numéricos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1OVaNED56Fz"
      },
      "source": [
        "Para este ejemplo usaremos “*Character level language model*” propuesto por Andrej Karpathy en su artículo \"*The Unreasonable Effectiveness of Recurrent Neural Networks*\"(y parcialmente basado en su implementado en el tutorial \"*Generate text with an RNN*\" de la web de TensorFlow:\n",
        "\n",
        "Consiste en darle a la RNN una palabra y se le pide que modele la distribución de probabilidad del siguiente carácter que le correspondería a la secuencia de caracteres anteriores:\n",
        "\n",
        "Como ejemplo, supongamos que solo tenemos un vocabulario de cuatro letras posibles [“a”,”h”,”l”,”o”], y queremos entrenar a una RNN en la secuencia de entrenamiento “hola”. Esta secuencia de entrenamiento es, de hecho, una fuente de 3 ejemplos de entrenamiento por separado: La probabilidad de “o” debería ser verosímil dada el contexto de “h”, “l” debería ser verosímil en el contexto de “ho”, y finalmente “a” debería ser también verosímil dado el contexto de “hol”.\n",
        "\n",
        "\n",
        "---\n",
        "*   https://unipython.com/generacion-de-textos-con-inteligencia-artificial/\n",
        "*    https://medium.com/analytics-vidhya/solution-to-tensorflow-2-not-using-gpu-119fb3e04daa (https://www.youtube.com/watch?v=kaQCdv46OBA&ab_channel=JeffHeaton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6zNNLvrPR1f"
      },
      "source": [
        "#EJEMPLO 1: Prediciendo un texto (La biblioteca de label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znb5-a_wR2Bs"
      },
      "source": [
        "!kill -9 -1 #reiniciando la maquina virtual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVxWwESyPWDg"
      },
      "source": [
        "##P0. importando librerias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl0r6XTg4Jj7"
      },
      "source": [
        "###Librerias genericas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-gVYOjJ4h_W"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97eE8L5n4dxZ"
      },
      "source": [
        "import requests\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "#librerías para graficar\n",
        "import matplotlib.pyplot as plt\n",
        "                 \n",
        "plt.rcParams['figure.figsize'] = (16, 9)  #ver graficas grandes \n",
        "plt.style.use('ggplot')\n",
        "#guardar las imagenes y tablas en el cuaderno de jupyter\n",
        "%matplotlib inline "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQc-WNUY5QhI"
      },
      "source": [
        "###librerias para DL (Deep Learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcCanSaNPY4Y"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABOLhv9Y5Fuf",
        "outputId": "76802d40-7c73-444d-e973-0f4ddc62bc4d"
      },
      "source": [
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.7.0\n",
            "Eager mode:  True\n",
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcwUPBdNy8w_"
      },
      "source": [
        "Verificando los recursos de maquina para entrenar:\n",
        "\n",
        "---\n",
        "**CUDA (GPU-NVIDIA):**\n",
        "CUDA es una plataforma de computación paralela y un modelo de programación que hace que el uso de una GPU para la computación de propósito general sea simple y elegante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SJ4g06UyoZR"
      },
      "source": [
        "print(\"-------------------------------RAM------------------------------\")\n",
        "!cat /proc/meminfo  #cuanta memoria tenemos?\n",
        "print(\"-----------------------------PROCESADOR-------------------------\")\n",
        "!cat /proc/cpuinfo  # que procesador tenemos?\n",
        "print(\"-------------------------------CUDA-----------------------------\")\n",
        "!nvcc -V   # version de CUDA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkeq57R55V-W"
      },
      "source": [
        "###Activando la GPU\n",
        "\n",
        "---\n",
        "Logra aumentar la velocidad de entrenamiento en un 600% en PLN (RNN) y un 1000% en visión por computadores (CNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vieZASeN5KHd",
        "outputId": "8eac96a1-3d65-416b-826b-459c262d872b"
      },
      "source": [
        "tf.device('/GPU:0')# OJO: esto es para que tensorflow utilice la GPU (aumenta la velocidad de entrenamiento en un 600%) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f3688f12370>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjQUc0jjQVLR"
      },
      "source": [
        "\n",
        "##P1. Descarga y preprocesado de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d5wzxaBQW40"
      },
      "source": [
        "fileUrl     ='https://raw.githubusercontent.com/2HenryCardenas1/DeepLearning/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Datasets/la_biblioteca_de_babel.txt'\n",
        "fileContent = tf.keras.utils.get_file('la_biblioteca_de_babel.txt',fileUrl)\n",
        "texto       = open(fileContent, 'rb').read().decode(encoding='utf-8')\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikT-PEFlRc7y"
      },
      "source": [
        "##P2. pasar el texto a números\n",
        "\n",
        "---\n",
        "Sin importan el origen de la informacción (video, sonido, sensor, texto)...siempre debemos convertirlos en datos númericos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ok5t-tCmJK8",
        "outputId": "d06b1844-5f1e-4436-a4cb-17b38d633843"
      },
      "source": [
        "print(\"[!] Pasando todo el texto a minuscula\")\n",
        "texto_min =texto.lower()\n",
        "time.sleep(1)\n",
        "print(\"[!] Quitando los caracteres especiales del texto\")\n",
        "texto_editado = texto_min.replace('\\n', '').replace('\\r','\\n').replace('(','').replace(')','').replace('!','').replace('1','').replace('7','').replace('-','').replace('?','').replace('«','').replace('»','').replace('¿','').replace('¡','')\n",
        "texto_final=texto_editado.replace('\\n', '')\n",
        "time.sleep(1)\n",
        "print(\"Hecho !!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] Pasando todo el texto a minuscula\n",
            "[!] Quitando los caracteres especiales del texto\n",
            "Hecho !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16uNdDlRRg4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18833ebf-0764-4228-ee70-f1d62424dca9"
      },
      "source": [
        "chars = sorted(list(set(texto_final)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(chars)\n",
        "print(char_to_int)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', ',', '.', ':', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', 'ñ']\n",
            "{' ': 0, ',': 1, '.': 2, ':': 3, ';': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'j': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'x': 26, 'y': 27, 'z': 28, 'ñ': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU_SXe5JSXT0",
        "outputId": "b97c6bed-7911-40b0-ceb6-c5acf4f5a6b5"
      },
      "source": [
        "n_chars = len(texto_final)\n",
        "n_vocab = len(chars)\n",
        "print(\"En total hay %d caracteres, y el diccionario tiene un tamaño de %d caracteres.\" % (n_chars, n_vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En total hay 15631 caracteres, y el diccionario tiene un tamaño de 30 caracteres.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ucpDS5lTgKB"
      },
      "source": [
        "###P2.1 Dividimos el texto en secuencias:\n",
        "---\n",
        "Dividimos el texto en estas secuencias (adrede), convertimos los caracteres a números enteros usando nuestra tabla de búsqueda que preparamos anteriormente\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJi9bHtTI46"
      },
      "source": [
        "# preparar el conjunto de datos de los pares de entrada a salida codificados como enteros\n",
        "seq_length = 50   #largo de las secciones de texto que usaremos para entrenar\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tprint(\"Seq(\",i,\")=\",texto_final[i:i + seq_length],\"---->\",raw_text[i + seq_length])\n",
        "\tseq_in \t= texto_final[i:i + seq_length]         \t\t\t\t\t#secuencia de entrada\n",
        "\tseq_out = texto_final[i + seq_length]\t\t\t\t\t\t\t\t\t\t#siguiente letra despues de la secuencia (la que el va a aprender)\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])  # convertimos cada secuencia en numeros\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Se generaron \",format(n_patterns) ,\" secuencias texto de un tamaño de \",seq_length,\" caracteres\" )\n",
        "print(\"Como se ven los datos de X convertidos a números\\n\")\n",
        "print(dataX)\n",
        "print(\"\\nComo se ven los datos de Y convertidos a números\\n\")\n",
        "print(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBlU-qNzVOO7"
      },
      "source": [
        "##P3. preparar nuestros datos de entrenamiento\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1.   Primero debemos transformar la lista de secuencias de entrada en la forma [muestras, pasos de tiempo, características] esperada por una red LSTM.\n",
        "2.   Luego debemos cambiar la escala de los números enteros al rango de 0 a 1 para que los patrones sean más fáciles de aprender mediante la red LSTM que utiliza la función de activación sigmoidea de forma predeterminada.\n",
        "3.   por ultimo necesitamos convertir los patrones de salida (caracteres individuales convertidos en enteros) en una codificación one hot. Esto es para que podamos configurar la red para predecir la probabilidad de cada uno de los 54 caracteres diferentes en el vocabulario (una representación más fácil)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbY1cgrHVSjC"
      },
      "source": [
        "#transformar la lista X de secuencias de entrada en la forma [muestras (3091), pasos de tiempo, características]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalizar (cambiar la escala de los números enteros al rango de 0 a 1 )\n",
        "X = X / float(n_vocab)\n",
        "# convertir los patrones de salida (caracteres individuales convertidos en enteros) en una codificación one hot.\n",
        "y = tf.keras.utils.to_categorical(dataY)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekQyJUbvWvpZ"
      },
      "source": [
        "##P4.Construcción del modelo RNN\n",
        "\n",
        "---\n",
        "definimos nuestro modelo LSTM: \n",
        "Aquí definimos una única capa LSTM oculta con 256 unidades de memoria. La red usa deserción con una probabilidad de 20. La capa de salida es una capa densa que usa la función de activación softmax para generar una predicción de probabilidad para cada uno de los 54 caracteres entre 0 y 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sjIOgf8XE_g"
      },
      "source": [
        "# define the LSTM model\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(256, input_shape=(X.shape[1], X.shape[2])))    #creamos una capa con 256 unidades de memoria\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))            #Softmax convierte un vector de valores en una distribución de probabilidad para cada uno de los 39\n",
        "                                                                              #Softmax se utiliza a menudo como la activación para la última capa de una red de clasificación\n",
        "#utilizamos el algoritmo de optimización de ADAM para la velocidad\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h3kjfw4XikQ"
      },
      "source": [
        "###P4.1 Creando chekpoints\n",
        "\n",
        "---\n",
        "\n",
        "La red es lenta de entrenar (alrededor de 300 segundos por época) teniendo activa la GPU, ASí que crearemos CHECKPOINTS (puntos de control) para registrar todos los pesos de la red para archivar cada vez que se observe una mejora en la pérdida al final de la época. Usaremos el mejor conjunto de pesos (menor pérdida) para instanciar nuestro modelo generativo en la siguiente sección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXY_7xXlnwfv",
        "outputId": "af4653cf-4d41-4f99-c539-6841b7f1b304"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcvIQPOHXzYL"
      },
      "source": [
        "# definimos  una carpeta para guardar los checkpoint\n",
        "filepath=\"/content/drive/MyDrive/CheckPoints(prediccion de texto de un libro)/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FczzMAAEX-6h"
      },
      "source": [
        "###P4.2 entrenando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPSD1VSHYAJH"
      },
      "source": [
        "history = model.fit(X, y, epochs=1000, batch_size=128, verbose=1,callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWfRY8pPYMYp"
      },
      "source": [
        "##P5.Generando texto con una red LSTM\n",
        "\n",
        "\n",
        "---\n",
        "Vamos a cargar el ultimo CHECKPOINT de entrenamiento y con el haremos MAGIA!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngH34tw2YVB6"
      },
      "source": [
        "filename = \"/content/drive/MyDrive/CheckPoints(prediccion de texto de un libro)/weights-improvement-947-0.0177.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoKm0kDwY5XG"
      },
      "source": [
        "###P5.1 mapeo inverso (números a letras)\n",
        "creamos un mapeo inverso que podamos usar para convertir los números enteros nuevamente en caracteres para que podamos entender las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEuAaVxpY-ro"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R9T2vJ6ZU9t"
      },
      "source": [
        "###P5.2 hacer predicciones\n",
        "La forma más sencilla de utilizar el modelo Keras LSTM para hacer predicciones es comenzar primero con una secuencia semilla como entrada, generar el siguiente carácter y luego actualizar la secuencia semilla para agregar el carácter generado al final y recortar el primer carácter. Este proceso se repite mientras queramos predecir nuevos caracteres (por ejemplo, una secuencia de 1000 caracteres de longitud)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEJmnxm6ZenD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce14a71-0890-4375-b34f-539642e9c13a"
      },
      "source": [
        "# elige una semilla al azar\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Semilla:\")\n",
        "print (\"\\\"\" + \" \".join([int_to_char[value] for value in pattern])+\"\\\"\")\n",
        "# generación de 10000 caracteres\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semilla:\n",
            "\"o ,   n i n g u n a   m e n t e   r a z o n a b l e   p u e d e   d u d a r .   e l   h o m b r e ,\"\n",
            " elimperfecto bibliotecario, puede ser obra del azar o de los demiurgos malevolos; eluniverso, con su elegante dotacion de anaqueles, de tomos enigmaticos, de infatigablesescaleras para el viajero y de letrinas para el bibliotecario sentado, solo puede ser obra deun dios. para percibir la distancia que hay entre lo divino y lo humano, basta comparar estos rudos simbolos tremulos que mi falible mano garabatea en la tapa de un libro, conlas letras organicas del interior: puntuales, delicadas, negrisimas, inimitablementesimetricas.el segundo: el numero de simbolos ortograficos es veinticinco. esa comprobacionpermitio, hace trescientos años, formular una teoria general de la biblioteca y resolversatisfactoriamente el problema que ninguna conjetura habia descifrado: la naturalezainforme y caotica de casi todos los libros. uno, que mi padre vio en un hexagono delcircuito quince noventa y cuatro, constaba de las letras mcv perversamente repetidasdesde el renglon primero hasta el ultimo. otro \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRh4YwjBVmvx"
      },
      "source": [
        "##P6.Mejorando la red (una LSTM más grande)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z5w5CleVu6j"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.LSTM(256))                                                                #agregaremos una segunda capa. \n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "#cambiamos el nombre de archivo de los pesos con puntos de control para que \n",
        "#podamos distinguir entre los pesos de esta red \n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGLMqyQvWRRv"
      },
      "source": [
        "###P6.1 mejoramos el entrenamiento\n",
        "\n",
        "---\n",
        "aumentamos las epoch y disminuiremos el tamaño del lote de 128 a 64 para darle a la red más oportunidades de actualizarse y aprender.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kjWAw-JWVce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c73e562-4153-4474-9c0d-0376412ea3be"
      },
      "source": [
        "#los tiempos de entrenamiento aumentaran al doble que en la versión anterior\n",
        "model.fit(X, y, epochs=50, batch_size=32, callbacks=callbacks_list)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f360a6a1c10>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TOM8C2_Y2To"
      },
      "source": [
        "###P6.2 haciendo predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFEPBYrxY5xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77b77f5-7052-44c3-fc47-8d6a49544d47"
      },
      "source": [
        "# elige una semilla al azar\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Semilla:\")\n",
        "print (\"\\\"\" + \" \".join([int_to_char[value] for value in pattern])+\"\\\"\")\n",
        "# generación de caracteres\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semilla:\n",
            "\"q u e   d a   t o d a   l a v u e l t a   d e   l a s   p a r e d e s ;   p e r o   s u   t e s t i\"\n",
            "doen los hombres de libros.ee eormato una soda lavualis del uo hexagono del cumto de esefuncionario remoto. muchos peregrinadas en las letrinas, con unos discos dem comentario de ese evangelio, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia de esosc\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz7aR7_eiYGg"
      },
      "source": [
        "##P7. exportar modelo RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU7_ao0Ciibo",
        "outputId": "fceb2333-8976-4955-830d-954aa81f7a40"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbuFNARNisdA",
        "outputId": "ae31d803-8619-4b9e-d558-c0efc3f5df06"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import os\n",
        "# Serializamos el modelo en forma JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"modelRNN_cuentos.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/MyDrive/Models(RNN)/modeloRNN_cuento_la_biblioteca_de_babel.hdf5\")\n",
        "model.save('/content/drive/MyDrive/Models(RNN)/modelRNN__cuento_la_biblioteca_de_babel_v1_h5.h5')\n",
        "print(\"modelo salvado en disco\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo salvado en disco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVlqHaeS3qIf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FshwyfFykB_4"
      },
      "source": [
        "###P7.1 cargando un modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2BD6XFKkEmg"
      },
      "source": [
        "# Recrea exactamente el mismo modelo solo desde el archivo\n",
        "new_model = tf.keras.models.load_model('/content/modelRNN__cuento_la_biblioteca_de_babel_v1_h5.h5')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjz0OYUYlO2B",
        "outputId": "deaa75bf-64ef-4a26-bc9d-9b07dd321e64"
      },
      "source": [
        "chars = sorted(list(set(\"ninguna mente razonable puede dudar\")))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"En total hay %d caracteres, y el diccionario tiene un tamaño de %d caracteres.\" % (n_chars, n_vocab))\n",
        "pattern = dataX[5]\n",
        "print(pattern)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En total hay 15876 caracteres, y el diccionario tiene un tamaño de 16 caracteres.\n",
            "[13, 25, 9, 21, 22, 18, 0, 20, 24, 9, 0, 18, 23, 21, 18, 22, 0, 15, 15, 5, 16, 5, 17, 0, 15, 5, 0, 6, 13, 6, 15, 13, 18, 23, 9, 7, 5, 0, 22, 9, 0, 7, 18, 16, 19, 18, 17, 9, 0, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5z_PSwlkoGg",
        "outputId": "a5800290-fb49-4e88-8de2-19c8b29b1bfe"
      },
      "source": [
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Semilla:\")\n",
        "print (\"\\\"\" + \" \".join([int_to_char[value] for value in pattern])+\"\\\"\")\n",
        "# generación de caracteres\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = new_model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semilla:\n",
            "\"a   d i s t r i b u c i o n   d e   l a s   g a l e r i a s   e s   i n v a r i a b l e .   v e i n\"\n",
            "ebio dasagooo laromalam; anio deebanadan daxagsoebres y qo pilpeeser; eletoe anes p mi miopegmesa ened q li eesorabenas, po hredi, fi heguil desari mo te jo freo, pi mi mi lievera l li melilebiean devaroaloe s mi peliamear; cayagso li midbresio faguaro mue mi megular m euedsearoo fagagsoras.m m mimpearo anasio faroras m qi mi mi milpearo q mi mi mi mreeso s mi milpefsesa eleduerar, earoiribnebroo davagooo lamimabes detanmabas domoralidoebreo, ieo, ni me seloe  lo frearo anis deebananan n mi memilen q qo lideeeeser; y gamomamam;bas docara liliiioebanoes;r des he mi li fimoaf q li lilpesmas davagsoo laromalam; anioi canagrororid s li lidbmearoo q mieperman m euedeees. ca piamisisiribccua y lamomebanoearoo v lamomecacooo davag t mi miever, mi ne milueano daxagoro laromalamaaariiiiepitibde, moe eoso v lamamearoableo;n n datoribarooalas,m moeeeeler; hoeduer; v to eioo fatoraboeoas, po pe melaman; hue hue s li meliiebie cnmananio datagooo lamimabes defelioibbcan de iieciiiiris defenioes q mu\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHu5fIzX9Xlp"
      },
      "source": [
        "#Ejemplo 2: generar texto de cuentos, usando Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5WVmMwYt7Uk"
      },
      "source": [
        "##P0. importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJdVpcLu7R7y"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGa1tvf5t5r4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYzaL8aex1Lz"
      },
      "source": [
        "uso de GPU para entrenar en tensorflow\n",
        "\n",
        "---\n",
        "https://medium.com/analytics-vidhya/solution-to-tensorflow-2-not-using-gpu-119fb3e04daa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlRSrBJrK9T",
        "outputId": "9714e4d3-a936-4fbe-b51f-0767a539dbbe"
      },
      "source": [
        "print(\"Version: \", tf.__version__)\n",
        "#print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU esta\", \"disponible\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
        "print(\"Dispositivos disponibles: \", tf.config.list_physical_devices())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.7.0\n",
            "GPU esta disponible\n",
            "Dispositivos disponibles:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9E2ihO2rLjM",
        "outputId": "f1be5384-fadc-452e-e3da-af261089e6d5"
      },
      "source": [
        "#tf.device('/gpu:0') #activando la CPU\n",
        "tf.device('/GPU:0') #activando la GPU "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f35e1ff62d0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ot0vNyP9jlQ"
      },
      "source": [
        "##P0. Descarga y preprocesado de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PETkDFl3Mff"
      },
      "source": [
        "fileDL= tf.keras.utils.get_file('la_biblioteca_de_babel.txt','https://raw.githubusercontent.com/2HenryCardenas1/DeepLearning/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Datasets/la_biblioteca_de_babel.txt')\n",
        "texto = open(fileDL, 'rb').read().decode(encoding='utf-8')\n",
        "texto = texto.lower()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Qr24Chypjd"
      },
      "source": [
        "##P1. entendiendo el texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oqDUlPuep9R"
      },
      "source": [
        "Normalizando el texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvHxVOhZepFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea6b320-b974-40c8-8204-f9d5e83e52cf"
      },
      "source": [
        "print(\"[!] Pasando todo el texto a minuscula\")\n",
        "texto_min =texto.lower()\n",
        "time.sleep(1)\n",
        "print(\"[!] Quitando los caracteres especiales del texto\")\n",
        "texto_editado = texto_min.replace('\\n', '').replace('\\r','\\n').replace('(','').replace(')','').replace('!','').replace('1','').replace('7','').replace('-','').replace('?','').replace('«','').replace('»','').replace('¿','').replace('¡','')\n",
        "texto_final=texto_editado.replace('\\n', '')\n",
        "time.sleep(1)\n",
        "print(\"Hecho !!\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] Pasando todo el texto a minuscula\n",
            "[!] Quitando los caracteres especiales del texto\n",
            "Hecho !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxHpVkcys57",
        "outputId": "7227eef5-f8e3-4d82-9352-a1ab7753d139"
      },
      "source": [
        "print('el texto tiene longitud de:{} caracteres'. format(len(texto_final)))\n",
        "vocab = sorted(set(texto_final))\n",
        "print('el texto esta compuesto de estos :{} caracteres'. format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el texto tiene longitud de:15631 caracteres\n",
            "el texto esta compuesto de estos :30 caracteres\n",
            "[' ', ',', '.', ':', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', 'ñ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPUI-QIDy7Mm"
      },
      "source": [
        "##P2. pasar el texto a números\n",
        "\n",
        "---\n",
        "as redes neuronales solo procesan valores numéricos, no letras, por tanto tenemos que traducir los caracteres a representación numérica. Para ello crearemos dos “tablas de traducción”: una de caracteres a números y otra de números a caracteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUZb1tLVzIke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4d6117-9138-40f2-fb09-857eb702d262"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)} # asignamos un número a cada vocablo\n",
        "idx2char = np.array(vocab)\n",
        "#-----------revisando las conversiones\n",
        "#for char,_ in zip(char2idx, range(len(vocab))):\n",
        "#    print(' {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n",
        "\n",
        "#pasamos todo el texto a números\n",
        "texto_como_entero= np.array([char2idx[c] for c in texto_final])\n",
        "print('texto: {}'.format(repr(texto_final[:100])))\n",
        "print('{}'.format(repr(texto_como_entero[:100])))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texto: 'el universo que otros llaman la biblioteca se compone de un numero indefinido, ytal vez infinito, de'\n",
            "array([ 9, 15,  0, 24, 17, 13, 25,  9, 21, 22, 18,  0, 20, 24,  9,  0, 18,\n",
            "       23, 21, 18, 22,  0, 15, 15,  5, 16,  5, 17,  0, 15,  5,  0,  6, 13,\n",
            "        6, 15, 13, 18, 23,  9,  7,  5,  0, 22,  9,  0,  7, 18, 16, 19, 18,\n",
            "       17,  9,  0,  8,  9,  0, 24, 17,  0, 17, 24, 16,  9, 21, 18,  0, 13,\n",
            "       17,  8,  9, 10, 13, 17, 13,  8, 18,  1,  0, 27, 23,  5, 15,  0, 25,\n",
            "        9, 28,  0, 13, 17, 10, 13, 17, 13, 23, 18,  1,  0,  8,  9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIT7JzLG4LIs"
      },
      "source": [
        "##P3. preparar los datos para ser usados en la RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky_cT7xN4OiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3702645a-9ac4-4b2b-ab77-a286284c90a9"
      },
      "source": [
        "char_dataset= tf.data.Dataset.from_tensor_slices(texto_como_entero)\n",
        "#cantidad de secuencia de caracteres\n",
        "secu_length=150\n",
        "#creamos secuencias de maximo 100 caractereres\n",
        "secuencias= char_dataset.batch(secu_length+1, drop_remainder=True)\n",
        "for item in secuencias.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'el universo que otros llaman la biblioteca se compone de un numero indefinido, ytal vez infinito, de galerias hexagonales, con vastos pozos de ventilac'\n",
            "'ion en el medio,cercados por barandas bajisimas. desde cualquier hexagono se ven los pisos inferiores ysuperiores: interminablemente. la distribucion d'\n",
            "'e las galerias es invariable. veinteanaqueles, a cinco largos anaqueles por lado, cubren todos los lados menos dos; su altura,que es la de los pisos, e'\n",
            "'xcede apenas la de un bibliotecario normal. una de las caras libresda a un angosto zaguan, que desemboca en otra galeria, identica a la primera y a tod'\n",
            "'as. aizquierda y a derecha del zaguan hay dos gabinetes minusculos. uno permite dormir depie; otro, satisfacer las necesidades finales. por ahi pasa la'\n",
            "' escalera espiral, que se abismay se eleva hacia lo remoto. en el zaguan hay un espejo, que fielmente duplica lasapariencias. los hombres suelen inferi'\n",
            "'r de ese espejo que la biblioteca no es infinita silo fuera realmente a que esa duplicacion ilusoria; yo prefiero soñar que las superficiesbruñidas fig'\n",
            "'uran y prometen el infinito... la luz procede de unas frutas esfericas quellevan el nombre de lamparas. hay dos en cada hexagono: transversales. la luz'\n",
            "' queemiten es insuficiente, incesante.como todos los hombres de la biblioteca, he viajado en mi juventud; he peregrinadoen busca de un libro, acaso del'\n",
            "' catalogo de catalogos; ahora que mis ojos casi no puedendescifrar lo que escribo, me preparo a morir a unas pocas leguas del hexagono en quenaci. muer'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz5cEwf6_bpv"
      },
      "source": [
        "###P3.1 separar los datos en agrupamientos (batches)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l6Tobzz7hww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4eb0917-9403-445e-a22b-4951f992fdd0"
      },
      "source": [
        "#funcion para obtener el conjunto de datos de trainning\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text= chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset  = secuencias.map(split_input_target)\n",
        "#el dataset contiene un conjunto de parejas de secuencia de texto\n",
        "#(con la representación numérica de los caracteres), donde el \n",
        "#primer componente de la pareja contiene un paquete con una secuencia \n",
        "#de 100 caracteres del texto original y la segunda su correspondiente salida, \n",
        "#también de 100 caracteres. )\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data:  'el universo que otros llaman la biblioteca se compone de un numero indefinido, ytal vez infinito, de galerias hexagonales, con vastos pozos de ventila'\n",
            "Target data:  'l universo que otros llaman la biblioteca se compone de un numero indefinido, ytal vez infinito, de galerias hexagonales, con vastos pozos de ventilac'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UM54Sfe9x80",
        "outputId": "6fe950fd-0b13-43b3-d06d-7a2eba8cf469"
      },
      "source": [
        "#imprimimos el tensor del dataset\n",
        "print(dataset)\n",
        "#Hyper-Parametros para entrenamiento  de una rede neuronal \n",
        "#   -los datos se agrupan en batch\n",
        "BATCH_SIZE= 64\n",
        "#    -Tamaño de memoria disponible \n",
        "BUFFER_SIZE=10000\n",
        "dataset= dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print (dataset)\n",
        "#En el tensor dataset disponemos los datos de entrenamiento\n",
        "#con agrupamienttos (batches) compuestos de 64 parejas de secuencias \n",
        "#de 100 integers de 64 bits que representan el carácter correspondiente \n",
        "#en el vocabulario."
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((150,), (150,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFFru4s_jon"
      },
      "source": [
        "##P4.Construcción del modelo RNN\n",
        "\n",
        "---\n",
        "Para construir el modelo usaremos tf.keras.Sequential. Usaremos una versión mínima de RNN, que contenga solo una capa LSTM y 3 capas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox_5lKZh_qUN"
      },
      "source": [
        "#como es un problema de clasificación estándar \n",
        "#para el que debemos definir la función de Lossy el optimizador.\n",
        "def lossy(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  #creando el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  #En cuanto al optimizador usaremos tf.keras.optimizers.Adam \n",
        "  #con los argumentos por defecto del optimizador Adam. \n",
        "  model.compile(optimizer='adam',\n",
        "              loss=lossy,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "vocab_size= len(vocab)\n",
        "#dimensiones de los vectores que tendrá la capa.\n",
        "embedding_dim= 256\n",
        "#cantidad de neuronas\n",
        "rnn_units=1024\n",
        "#creamos nuestra red neuronal RNN\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "#summary()para visualizar la estructura del modelo\n",
        "model.summary()\n",
        "#resultados=  -La capa LSTM consta más de 5 millones de parametros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnll1ubFvZO"
      },
      "source": [
        "###P4.1 Creando chekpoints\n",
        "\n",
        "---\n",
        "una técnica de tolerancia de fallos para procesos cuyo tiempo de ejecución es muy largo. La idea es guardar una instantánea del estado del sistema periódicamente para recuperar desde ese punto la ejecución en caso de fallo del sistema.\n",
        "\n",
        "---\n",
        "los crearemos en google drive para mejorar la capacidad de reentrenamiento de la red\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L24WhqeauMWk",
        "outputId": "1036649d-e2d4-40c1-839f-5fc6b855e3bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbeXxiWLF5hN"
      },
      "source": [
        "checkpoint_dir='/content/drive/MyDrive/CheckPoints(generando texto de cuentos)/'#direccion de la carpeta checkpoint\n",
        "checkpoint_prefix= os.path.join(checkpoint_dir,\"cp_{epoch:04d}.ckpt\")\n",
        "\n",
        "\n",
        "cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                               monitor='loss',\n",
        "                                               verbose=1,\n",
        "                                               save_weights_only=True,\n",
        "                                               save_best_only=True,\n",
        "                                               mode='auto')\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trmMLMjxGjuP"
      },
      "source": [
        "###P4.2 entrenando"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPXvQuLrQEAF"
      },
      "source": [
        "####P4.2a entrenando para usar chekpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5cnXgLQI7H"
      },
      "source": [
        "EPOCHS=1000 #Segun la cantidad de epoch que queramos seguir entrenando\n",
        "history=model.fit(dataset, \n",
        "                  epochs=EPOCHS, \n",
        "                  verbose=1,\n",
        "                  callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URtG1ZI0L99C"
      },
      "source": [
        "\n",
        "#####4.2a-1 entrenando desde un checkpoint\n",
        "\n",
        "---\n",
        "Desde la carpeta que optamos guardar los checkpoints\n",
        "\n",
        "*   el archivo .data es el archivo que contiene nuestras variables de entrenamiento y vamos a ir tras él.\n",
        "*   el archivo checkpoint, simplemente mantiene un registro de los últimos archivos de punto de control guardados\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o4h9cq8ac68",
        "outputId": "983ea428-1e23-49cd-ba18-f0eb34776ebe"
      },
      "source": [
        "#creamos un modelo con iguales caracteristicas al 1° modelo\n",
        "model2=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "\n",
        "#buscamos el ultimo checkpoint de entrenamiento\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Redes neuronales recurrentes (RNN)/checkpointsV3/cp_0030.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGZCz4W3lkWa"
      },
      "source": [
        "# cargamos los pesos al nuevo modelo (estos valores tienes una variación de un 10%)\n",
        "model2.load_weights(latest)\n",
        "# continuamos el entrenamiento desde el checkpoint en que quedamos\n",
        "history_V2=model2.fit(dataset, \n",
        "                    epochs=100, \n",
        "                    verbose=1,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIkOBKGAP0y-"
      },
      "source": [
        "####P4.2b entrenando con tensorboard (opcional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFDzT0NtUpua"
      },
      "source": [
        "#####Activando TENSORBOARD \n",
        "\n",
        "---\n",
        "(DASHBOARD para ver el proceso de entrenamiento)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x20xRy-UwzZ"
      },
      "source": [
        "# You can change the directory name\n",
        "LOG_DIR = '/content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Redes neuronales recurrentes (RNN)/checkpoints'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ1L-Vm1ZrTU",
        "outputId": "e8e0d6b6-e124-46f1-9ab9-481459318cca"
      },
      "source": [
        "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, \n",
        "                         histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         write_images=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmC4IO8rg5TV"
      },
      "source": [
        "#####Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcLIbeJjGmDF"
      },
      "source": [
        "\n",
        "#model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIbqMyqzrWU"
      },
      "source": [
        "##P5. Generando texto nuevo usando la RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vGr7GvUzwxg"
      },
      "source": [
        "#creamos un modelo tomando como base el ultimo checkpoint\n",
        "model = create_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))##tomamos el ultimo checkpoint\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvoSiWWp2_H-"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores/cantidad de texto generaremos\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval= tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.3  #(0.0 a  1) entre más alta la temperatura más creatividad al modelo, pero tambien más errores ortograficos.\n",
        "  model.reset_states() #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFfQ5EiP4ghF"
      },
      "source": [
        "###P5.1 generando texto "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkLdbX724kBy",
        "outputId": "a32546f4-2eb0-4d45-fa50-8bff4e9fa4bd"
      },
      "source": [
        "print(generate_text(model, start_string=u\"la colonia europea\"))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "la colonia european que las parie el desprico de aque en lugares remotos los corredores y escaleras y hexagonos puedeninconcebiblemente cesar, lo cual es absurdo. quienes la imaginan sin limites, olvide catalogos falsos, la demostracion de la falacia de esoscatalogos, la demostracion de la falacia del catalogo verdadero, el evangelio gnostico debasiguna del cutron todas las pombres scoren queeno deplicado se alobra no un selor en esas.otrados enoraque el dios hexagonoles, invar able denes, de nuestra intuicionde \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-3PF45LpJBS"
      },
      "source": [
        "##P6.exportando modelo\n",
        "\n",
        "---\n",
        "Guardamos y Serializamos el Modelo (con esto ya podemos vender nuestro modelo de predicción de texto según lo aprendido por nuestra RNN).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_BJSb-7pL7B",
        "outputId": "b5814468-2363-4cd4-db7a-1f13714a9179"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import os\n",
        "dir_export= '/content/drive/MyDrive/Models(RNN)' #Ruta del directorio el cual vamos a guardar el modelo\n",
        "#dir_export= os.path.join(dir_drive)\n",
        "# Serializamos el modelo en forma JSON\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(dir_export,'RNN_LaBibliotecaDeBabel_json.json'), 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(os.path.join(dir_export,'RNN_LaBibliotecaDeBabel_pesos.hdf5'))\n",
        "model.save(os.path.join(dir_export,'RNN_LaBibliotecaDeBabel_model.h5'))\n",
        "print(\"modelo salvado en Drive de google\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo salvado en Drive de google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfogT546zoDp"
      },
      "source": [
        "##P7.Cargando un modelo serializado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d_bhKoHCE7m"
      },
      "source": [
        "###P7.1 descargamos el modelo usando wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en0LdkesfPJE"
      },
      "source": [
        "Ponemos la url del"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R11dLzBzwv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb1f673-799d-46b2-df9b-9c15cb4b1b8e"
      },
      "source": [
        "!wget 'https://github.com/2HenryCardenas1/DeepLearning/blob/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5?raw=true' -O RNN_LaBibliotecaDeBabel_model.h5\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-19 16:25:04--  https://github.com/2HenryCardenas1/DeepLearning/blob/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/2HenryCardenas1/DeepLearning/raw/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5 [following]\n",
            "--2021-11-19 16:25:05--  https://github.com/2HenryCardenas1/DeepLearning/raw/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/2HenryCardenas1/DeepLearning/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5 [following]\n",
            "--2021-11-19 16:25:05--  https://raw.githubusercontent.com/2HenryCardenas1/DeepLearning/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63459344 (61M) [application/octet-stream]\n",
            "Saving to: ‘RNN_LaBibliotecaDeBabel_model.h5’\n",
            "\n",
            "RNN_LaBibliotecaDeB 100%[===================>]  60.52M   160MB/s    in 0.4s    \n",
            "\n",
            "2021-11-19 16:25:08 (160 MB/s) - ‘RNN_LaBibliotecaDeBabel_model.h5’ saved [63459344/63459344]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKuJocQwCLg1"
      },
      "source": [
        "####P7.1a descargamos el modelo usando PYRIND & URLLIB (OPCIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w_jNNbCAul4",
        "outputId": "4fec3b78-d2b2-45c8-fc4d-8872bfc75580"
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZcueKBiAqRd",
        "outputId": "856eadef-35ec-4ea6-8b6c-415996930b43"
      },
      "source": [
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d segundos transcurrido\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "import urllib.request\n",
        "url_github_Model='https://github.com/2HenryCardenas1/DeepLearning/blob/main/Jupyter/PLN(Procesamiento_de_lenguajes_naturales)/Models/RNN_LaBibliotecaDeBabel_model.h5?raw=true'\n",
        "urllib.request.urlretrieve(url_github_Model,\n",
        "                           'RNN_LaBibliotecaDeBabel_model2.h5', \n",
        "                           reporthook)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% | 60 MB | 3.53 MB/s | 17 segundos transcurrido"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RNN_LaBibliotecaDeBabel_model2.h5',\n",
              " <http.client.HTTPMessage at 0x7f35e1178c50>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lxBCpidCk7b"
      },
      "source": [
        "###P7.2 instanciamos el modelo descargado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYYSyuE6DHj_"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dewzz_pW9fFE"
      },
      "source": [
        "new_model = keras.models.load_model('/content/RNN_LaBibliotecaDeBabel_model2.h5') #Nombre del modelo descargado"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-rwH5vd0oGc"
      },
      "source": [
        "print(generate_text(new_model, start_string=u\"colombia es un pais \")) #palabra "
      ],
      "execution_count": 81,
      "outputs": []
    }
  ]
}